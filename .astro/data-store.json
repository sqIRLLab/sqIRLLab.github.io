[["Map",1,2,9,10,86,87,308,309,424,425,496,497],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.3","content-config-digest","e85c5da6148dd422","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://sqirllab.github.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","people",["Map",11,12,21,22,30,31,40,41,49,50,59,60,68,69,77,78],"arian",{"id":11,"data":13,"filePath":19,"digest":20},{"name":14,"status":15,"specialty":16,"image":17,"link":18},"Arian Sabaghi","PhD student","Weakly-supervised object localization","arian.jpg","https://scholar.google.com/citations?user=xEsTYSQAAAAJ&hl=en","src/content/people/arian.json","39fb02e641effa9c","benjamin",{"id":21,"data":23,"filePath":28,"digest":29},{"name":24,"status":15,"specialty":25,"image":26,"link":27},"Benjamin Vandersmissen","Representations and lottery ticket hypothesis","benjamin.jpg","https://scholar.google.com/citations?user=pfgVNMUAAAAJ&hl=en","src/content/people/benjamin.json","5463b1cb0277467e","jose",{"id":30,"data":32,"filePath":38,"digest":39},{"name":33,"status":34,"specialty":35,"image":36,"link":37},"Jose Oramas","Professor","Representation Learning and Interpretability","jose.png","https://idlab.uantwerpen.be/~joramasmogrovejo/","src/content/people/jose.json","422a134df86afd3b","peter",{"id":40,"data":42,"filePath":47,"digest":48},{"name":43,"status":15,"specialty":44,"image":45,"link":46},"Peter Kirby","Binary hyperdimensional computing","peter.jpg","https://www.uantwerpen.be/en/staff/peter-kirby_27404/","src/content/people/peter.json","89803df955ca2798","renata",{"id":49,"data":51,"filePath":57,"digest":58},{"name":52,"status":53,"specialty":54,"image":55,"link":56},"Renata Turke≈°","Postdoctoral researcher","Topological data analysis","renata.png","https://renata-turkes.github.io","src/content/people/renata.json","31038571fc01c248","saja",{"id":59,"data":61,"filePath":66,"digest":67},{"name":62,"status":15,"specialty":63,"image":64,"link":65},"Saja Tawalbeh","Explainable artificial intelligence","saja.png","https://stawalbeh.github.io/STawalbeh/","src/content/people/saja.json","7b704b9f71cf27e7","salma",{"id":68,"data":70,"filePath":75,"digest":76},{"name":71,"status":15,"specialty":72,"image":73,"link":74},"Salma Haidar","Hyperspectral image analysis","salma.jpg","https://www.salmahaidar.com/","src/content/people/salma.json","86f015ffd9df6b48","thomas",{"id":77,"data":79,"filePath":84,"digest":85},{"name":80,"status":15,"specialty":81,"image":82,"link":83},"Thomas Dooms","Weight-based interpretability","thomas.jpg","https://tdooms.github.io/","src/content/people/thomas.json","172ff8224d2951fd","publications",["Map",88,89,99,100,112,113,123,124,134,135,145,146,155,156,164,165,175,176,186,187,197,198,206,207,218,219,228,229,239,240,248,249,257,258,268,269,279,280,289,290,300,301],"action-recognition",{"id":88,"data":90,"filePath":97,"digest":98},{"title":91,"date":92,"conference":93,"tags":94,"link":95,"image":26,"description":96},"Recognizing actions in high-resolution low-framerate videos: a feasibility study in the construction sector","2024/02/27","VISAPP'24",[],"https://repository.uantwerpen.be/docman/irua/3d6ebamotoMe4","This study investigates the applicability of established action recognition methodologies in the dynamic setting of the construction sector.","src/content/publications/action-recognition.json","668528a1c5a528ca","bilinear-autoencoders",{"id":99,"data":101,"filePath":110,"digest":111},{"title":102,"date":103,"conference":104,"tags":105,"link":108,"image":82,"description":109},"Finding manifolds with bilinear autoencoders","2025/10/01","NeurIPS'25",[106,107],"spotlight","workshop","https://www.arxiv.org/pdf/2510.16820","Decomposing activations into sparse polynomials and using their geometry","src/content/publications/bilinear-autoencoders.json","070632f83d3b8ac1","bilinear-mlps",{"id":112,"data":114,"filePath":121,"digest":122},{"title":115,"date":116,"conference":117,"tags":118,"link":119,"image":82,"description":120},"Bilinear MLPs enable weight-based mechanistic interpretability","2024/10/01","ICLR'25",[106],"https://tdooms.github.io/research/bilinear","Using bilinear MLPs to reverse-engineer shallow MNIST and Tiny Stories models from their weights.","src/content/publications/bilinear-mlps.json","7816394688a86612","capsnet-representations",{"id":123,"data":125,"filePath":132,"digest":133},{"title":126,"date":127,"conference":128,"tags":129,"link":130,"image":64,"description":131},"Towards the Characterization of Representations Learned via Capsule-Based Network Architectures","2025/02/01","Neurocomputing",[],"https://www.sciencedirect.com/science/article/abs/pii/S0925231224017983","This paper provides a systematic and principled study on the interpretability of Capsule Network (CapsNet) representations, aiming to characterize the nature and structure of the learned features across diverse architectures and datasets","src/content/publications/capsnet-representations.json","ea4312975d82c3d0","coherence-explanations",{"id":134,"data":136,"filePath":143,"digest":144},{"title":137,"date":138,"conference":139,"tags":140,"link":141,"image":26,"description":142},"On the coherency of quantitative evaluation of visual explanations","2024/04/01","CVIU vol. 241",[],"https://doi.org/10.1016/j.cviu.2024.103934","Measuring the correlation between different quantitative evaluation metrics for visual explanations.","src/content/publications/coherence-explanations.json","da31296bc8404f3d","compositionality",{"id":145,"data":147,"filePath":153,"digest":154},{"title":148,"date":149,"conference":150,"tags":151,"image":82,"description":152},"Compositionality unlocks deep interpretable models","2024/12/01","AAAI'25",[107],"Introducing a global SVD-like algorithm for multi-linear models.","src/content/publications/compositionality.json","dc581920f7142f6c","hdc-interpretability",{"id":155,"data":157,"filePath":162,"digest":163},{"title":158,"date":103,"conference":128,"tags":159,"link":160,"image":36,"description":161},"Explaining and interpreting hyperdimensional computing classifiers on tabular data",[],"https://doi.org/10.1016/j.neucom.2025.131643","We make HDC classifiers for tabular data more interpretable","src/content/publications/hdc-interpretability.json","b9ccabdd3003e59a","hsi-contrastive",{"id":164,"data":166,"filePath":173,"digest":174},{"title":167,"date":168,"conference":169,"tags":170,"link":171,"image":73,"description":172},"A Contrastive Learning Method for Multi-Label Predictors on Hyperspectral Images","2024/02/01","WHISPERS",[],"https://doi.org/10.1109/WHISPERS61460.2023.10430726","Self-supervised contrastive learning for multi-label hyperspectral image classification","src/content/publications/hsi-contrastive.json","00db7829eb1f33b9","hsi-multilabel",{"id":175,"data":177,"filePath":184,"digest":185},{"title":178,"date":179,"conference":180,"tags":181,"link":182,"image":73,"description":183},"Training Methods of Multi-Label Prediction Classifiers for Hyperspectral Remote Sensing Images","2023/12/01","Remote Sensing",[],"https://doi.org/10.3390/rs15245656","A deep learning model for hyperspectral remote sensing, shifting from traditional single-label, pixel-level classification to multi-label, patch-level analysis","src/content/publications/hsi-multilabel.json","1a90715870c35626","matrix-capsnet",{"id":186,"data":188,"filePath":195,"digest":196},{"title":189,"date":190,"conference":191,"tags":192,"link":193,"image":64,"description":194},"Analyzing the Explanation and Interpretation Potential of Matrix Capsule Networks","2023/09/01","ECML-PKDD'23",[107],"https://link.springer.com/chapter/10.1007/978-3-031-74630-7_5#citeas","A look into the internal mechanisms of Matrix CapNets with EM Routing","src/content/publications/matrix-capsnet.json","31ad0bac216d7179","model-compression",{"id":197,"data":199,"filePath":204,"digest":205},{"title":200,"date":92,"conference":93,"tags":201,"link":202,"image":26,"description":203},"Deep learning model compression for resource efficient activity recognition on edge devices: a case study",[],"https://repository.uantwerpen.be/docman/irua/666490motoM98","This paper presents an approach to adapt an existing activity recognition model for efficient deployment on edge devices.","src/content/publications/model-compression.json","686d35973cb60458","radio-fingerprint",{"id":206,"data":208,"filePath":216,"digest":217},{"title":209,"date":210,"conference":211,"tags":212,"link":213,"image":214,"description":215},"Label-efficient learning for radio frequency fingerprint identification","2025/03/01","IEEE WCNC'25",[],"https://ieeexplore.ieee.org/abstract/document/10978444","fabian.jpg","We introduce a label-efficient approach for Radio Frequency Fingerprint Identification, achieving competitive accuracy with up to 10x fewer labels.","src/content/publications/radio-fingerprint.json","342b2c99bcf6afce","simplestories",{"id":218,"data":220,"filePath":226,"digest":227},{"title":221,"date":222,"conference":104,"tags":223,"link":224,"image":82,"description":225},"Parameterized Synthetic Text Generation with SimpleStories","2025/11/01",[],"https://openreview.net/pdf?id=sVh3eQ642W","A dataset full of simple yet diverse stories; the MNIST for language","src/content/publications/simplestories.json","1a094b6629126a32","smooth-infomax",{"id":228,"data":230,"filePath":237,"digest":238},{"title":231,"date":232,"conference":233,"tags":234,"link":235,"image":214,"description":236},"Smooth InfoMax - Towards Easier Post-Hoc Interpretability","2025/09/01","ECML-PKDD'25",[],"https://arxiv.org/abs/2408.12936","SIM makes post-hoc interpretability tools more effective through latent space constraints","src/content/publications/smooth-infomax.json","ecf8843894302409","svebi",{"id":239,"data":241,"filePath":246,"digest":247},{"title":242,"date":232,"conference":233,"tags":243,"link":244,"image":36,"description":245},"SVEBI: Towards the Interpretation and Explanation of Spiking Neural Networks",[],"https://link.springer.com/chapter/10.1007/978-3-032-06066-2_27","A posthoc explanation method for spiking neural networks","src/content/publications/svebi.json","54bb53a80b57b6a6","taxonomy-capsnet",{"id":248,"data":250,"filePath":255,"digest":256},{"title":251,"date":222,"conference":128,"tags":252,"link":253,"image":64,"description":254},"A Taxonomy of Interpretation and Explanation Methods for Capsule Network Architectures",[],"https://www.sciencedirect.com/science/article/abs/pii/S0925231225026979","This paper presents a comprehensive taxonomy of interpretation and explanation methods developed for Capsule Network (CapsNet) architectures, analyzing their mechanisms, applicability, and performance across diverse problem domains.","src/content/publications/taxonomy-capsnet.json","2a8ca3126666da83","tokenized-saes",{"id":257,"data":259,"filePath":266,"digest":267},{"title":260,"date":261,"conference":262,"tags":263,"link":264,"image":82,"description":265},"Tokenized SAEs: Disentangling SAE Reconstructions","2024/06/01","ICML'24",[107],"https://tdooms.github.io/research/tokenized","We use a per-token bias in SAEs to separate token reconstructions from interesting features.","src/content/publications/tokenized-saes.json","f5689c0034819c63","trifecta",{"id":268,"data":270,"filePath":277,"digest":278},{"title":271,"date":272,"conference":273,"tags":274,"link":275,"image":82,"description":276},"The Trifecta: Three techniques for deeper Forward-Forward networks","2023/11/01","TMLR",[],"https://tdooms.github.io/research/trifecta","Three techniques to significantly improve the Forward-Forward algorithm. We achieve 84% on CIFAR-10.","src/content/publications/trifecta.json","828a58fc61fa6322","twin-network-cnn",{"id":279,"data":281,"filePath":287,"digest":288},{"title":282,"date":283,"conference":117,"tags":284,"link":285,"image":26,"description":286},"Improving neural network accuracy by concurrently training with a twin network","2025/04/01",[],"https://openreview.net/forum?id=TEmE9PSC65","We show the applicability of twin network augmentation on convolutional neural networks","src/content/publications/twin-network-cnn.json","0d376d92ff975f98","twin-network-snn",{"id":289,"data":291,"filePath":298,"digest":299},{"title":292,"date":293,"conference":294,"tags":295,"link":296,"image":26,"description":297},"Twin Network Augmentation","2024/09/01","arXiv",[],"https://arxiv.org/pdf/2409.15849","A Novel Training Strategy for Improved Spiking Neural Networks and Efficient Weight Quantization.","src/content/publications/twin-network-snn.json","eb8ff15a76ecebbd","weight-based-decomposition",{"id":300,"data":302,"filePath":306,"digest":307},{"title":303,"date":261,"conference":262,"tags":304,"link":119,"image":82,"description":305},"Weight-based Decomposition: A Case for Bilinear MLPs",[107],"Introducing bilinear MLPs as a new approach to weight-based interpretability.","src/content/publications/weight-based-decomposition.json","f81a5cdaefd2f9de","events",["Map",310,311,319,320,327,328,335,336,344,345,353,354,362,363,371,372,380,381,389,390,397,398,406,407,415,416],"acrai-research-day",{"id":310,"data":312,"filePath":317,"digest":318},{"title":313,"date":314,"link":315,"image":73,"description":316},"Talk @ ACRAI Research Day","2025/02/21","https://www.uantwerpen.be/en/research-groups/acrai/","Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging","src/content/events/acrai-research-day.json","43c3f298c9329f0c","becode",{"id":319,"data":321,"filePath":325,"digest":326},{"title":322,"date":116,"link":323,"image":64,"description":324},"Talk @ BeCode","https://becode.org/","General presentation about explainability","src/content/events/becode.json","b704f32d0e63c0d0","clusity",{"id":327,"data":329,"filePath":333,"digest":334},{"title":330,"date":331,"link":332,"image":64,"description":324},"Talk @ Clusity","2023/06/01","https://www.clusity.be/","src/content/events/clusity.json","268cb0d7cd2ba870","dataset-distillation-vaia",{"id":335,"data":337,"filePath":342,"digest":343},{"title":338,"date":339,"link":340,"image":26,"description":341},"Webinar@Dataset Distillation - A Gentle Introduction","2025/07/15","https://www.vaia.be/en/courses/dataset-distillation-a-gentle-introduction","Get familiar with the standard dataset distillation techniques, and gain familiarity with uses cases that could benefit from it.","src/content/events/dataset-distillation-vaia.json","7f6e4e2768feca5e","efficientml",{"id":344,"data":346,"filePath":351,"digest":352},{"title":347,"date":348,"link":349,"image":26,"description":350},"Talk@Efficient ML Reading Group","2025/06/30","https://www.youtube.com/watch?v=rB2_SZIQUd0","Twin Network Augmentation for Convolutional Neural Networks.","src/content/events/efficientml.json","344416d884bcffd7","flanders-ai-2023",{"id":353,"data":355,"filePath":360,"digest":361},{"title":356,"date":357,"link":358,"image":26,"description":359},"Talk@Flanders AI Day - Workshop on Advances in Responsible AI","2023/10/16","https://www.flandersairesearch.be/en/research/research-day","Presentation on coherence between different evaluation metrics for visual explanations.","src/content/events/flanders-ai-2023.json","29f853d0238d6a70","flanders-ai-day",{"id":362,"data":364,"filePath":369,"digest":370},{"title":365,"date":366,"link":367,"image":82,"description":368},"Talk @ Flanders AI Day","2024/10/14","https://docs.google.com/presentation/d/1PdcHm0bgwIu3yhy6oWkwL1-fDGTPzTiyUilydognLlw/edit?usp=sharing","A swift introduction to weight-based interpretability.","src/content/events/flanders-ai-day.json","06b5b0d7783a0ace","kekule-cycle",{"id":371,"data":373,"filePath":378,"digest":379},{"title":374,"date":375,"link":376,"image":36,"description":377},"Talk @ Kekule Cycle XXI","2025/10/14","https://www.kvcv.be/nl/activiteiten-en-nieuws/kalender/676-quid-quantum-and-ai","An accessible introduction to representation learning and interpretability","src/content/events/kekule-cycle.json","5989a88e73d4f3db","vaia-webinar",{"id":380,"data":382,"filePath":387,"digest":388},{"title":383,"date":384,"link":385,"image":73,"description":386},"Webinar @ VAIA - Flanders AI EDIH HSI Talk","2025/09/29","https://www.vaia.be/","Hyperspectral Imaging and Machine Learning: Decoding the Spectrum for Advanced Data Analysis","src/content/events/vaia-webinar.json","70b281a9490e6153","visapp24",{"id":389,"data":391,"filePath":395,"digest":396},{"title":392,"date":92,"link":393,"image":26,"description":394},"Talk @ VisAPP'24","https://visapp.scitevents.org/","Recognizing actions in high-resolution low-framerate videos: a feasibility study in the construction sector.","src/content/events/visapp24.json","fed64b813f0b8092","wids",{"id":397,"data":399,"filePath":404,"digest":405},{"title":400,"date":401,"link":402,"image":64,"description":403},"Talk @ Women in Data Science (WiDS)","2024/03/01","https://www.widsconference.org/","Analyzing Representations Learned via Capsule Neural Networks. A systematic and principled study towards assessing the interpretability of Capsule networks","src/content/events/wids.json","934141e8fa1868c5","tl",{"id":406,"data":408,"filePath":413,"digest":414},{"title":409,"date":410,"link":411,"image":64,"description":412},"Webinar @ VAIA - Flanders AI EDIH Transfer Learning Talk","2025/10/22","https://events.teams.microsoft.com/event/c5a675b4-3d13-474d-b1f9-484d6d720e08@a72d5a72-25ee-40f0-9bd1-067cb5b770d4","From Experience to Expertise: An Introduction to Transfer Learning","src/content/events/tl.json","4969e53a53d2de75","ssl",{"id":415,"data":417,"filePath":422,"digest":423},{"title":418,"date":419,"link":420,"image":64,"description":421},"Webinar @ VAIA - Flanders AI EDIH Self-Supervised Learning Talk","2025/11/27","https://events.teams.microsoft.com/event/0e1ccef1-fae3-4ea4-a1c6-e8771b7cad3e@a72d5a72-25ee-40f0-9bd1-067cb5b770d4","Self-Supervised Learning: Training AI Models with Less Labels","src/content/events/ssl.json","b64ccbd989d1c32a","news",["Map",426,427,433,434,440,441,449,450,456,457,464,465,470,471,476,477,483,484,490,491],"2024-03-wids",{"id":426,"data":428,"filePath":431,"digest":432},{"date":401,"title":429,"description":430},"Event","Co-organized a Women in Data event.","src/content/news/2024-03-wids.json","5f73b4a7faa959ae","2024-09-workshop",{"id":433,"data":435,"filePath":438,"digest":439},{"date":293,"title":436,"description":437},"Workshop","Co-organized the AIMLAI Workshop held in conjunction with ECML-PKDD'24.","src/content/news/2024-09-workshop.json","7e2ee729f377a06a","2024-10-tmlr",{"id":440,"data":442,"filePath":447,"digest":448},{"date":443,"title":444,"description":445,"link":446},"2024/10/02","Paper accepted","Thomas' work on deeper forward-forward networks got published at TMLR.","https://openreview.net/forum?id=a7KP5uo0Fp","src/content/news/2024-10-tmlr.json","14183511984ccb85","2025-06-ecml",{"id":449,"data":451,"filePath":454,"digest":455},{"date":452,"title":444,"description":453},"2025/06/01","Two papers accepted at ECML-PKDD'25 on Smooth-InfoMax and Interpretability of SNNs.","src/content/news/2025-06-ecml.json","1aec7e1098ae188d","2025-09-renata",{"id":456,"data":458,"filePath":462,"digest":463},{"date":232,"title":459,"description":460,"link":461},"New member","sqIRL welcomes Renata to the lab.","https://renata-turkes.github.io/","src/content/news/2025-09-renata.json","d19570370e7284c3","2025-10-neurocomputing-hdc",{"id":464,"data":466,"filePath":468,"digest":469},{"date":103,"title":444,"description":467,"link":160},"One paper accepted at Neurocomputing on Interpretable HDC Classifiers.","src/content/news/2025-10-neurocomputing-hdc.json","02a93ace17aa8479","2025-11-neurocomputing",{"id":470,"data":472,"filePath":474,"digest":475},{"date":222,"title":444,"description":473,"link":253},"Saja's paper on positioning a taxonomy of interpretation and explanation methods for Capsule Networks was accepted in the journal Neurocomputing.","src/content/news/2025-11-neurocomputing.json","9aa17e92db1f2ddb","2024-12-neurocomputing",{"id":476,"data":478,"filePath":481,"digest":482},{"date":149,"title":444,"description":479,"link":480},"Saja's paper on the interpretability of Capsule Networks was accepted at the journal of Neurocomputing.","https://arxiv.org/abs/2305.05349","src/content/news/2024-12-neurocomputing.json","3cfb1c6745a2798b","2025-01-iclr",{"id":483,"data":485,"filePath":488,"digest":489},{"date":486,"title":444,"description":487},"2025/01/01","Two papers accepted at ICLR'25 on Twin Network Augmentation and Interpretability via Bilinear MLPs.","src/content/news/2025-01-iclr.json","39e899cda47d4191","2024-10-peter",{"id":490,"data":492,"filePath":494,"digest":495},{"date":116,"title":459,"description":493},"sqIRL welcomes Peter to the lab.","src/content/news/2024-10-peter.json","1cf43d89dbfe72a9","home",["Map",498,499],"main",{"id":498,"data":500,"filePath":506,"digest":507},{"title":501,"subtitle":502,"description":503,"email":504,"logo":505},"sqIRL","Interpretable Representation Learning","The Interpretable Representation Learning lab, sqIRL, at the University of Antwerp pursues fundamental research at the intersection of machine learning and interpretability/explainability. Our research focuses on the inner-workings of AI systems and the learning processes that produce them. We aim to develop AI systems that are interpretable/explainable and more efficient in their use of data and computational resources.","jose.oramasmogrovejo@uantwerpen.be","/logo.png","src/content/home/main.json","64c87d672340e3cb"]